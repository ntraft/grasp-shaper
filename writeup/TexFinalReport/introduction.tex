\renewcommand{\thesection}{\Roman{section}}
\thispagestyle{empty}
\section{Introduction}
\quad When humans perform simple grasping task in every day life, they depend on a combination of their visual system as well as their sensorimotor memory. Hereby, the human hand relies on about 17000 mechanoreceptive tactile units \cite{SkinTouch} embedded in the hairless skin of the palm that are able to give feedback in response to e.g. touch, pressure or vibrations, constantly adapting fingertip forces and grasping strength. Lifting up an object, such as a cup or a pen, is consequently followed by a cascade of sensory signal generation and processing \cite{TortaGerardJ.2011}. 

In humans, visual information of the objects properties during grasping is important, however not essential \cite{VisualSensory}. Consequently, a lot of research effort has been put into tactile-driven approaches for robotic grasp control \cite{LeeNicholls} \cite{Yousef1}. The main challenge remains yet to find a dexterous robotic grasping technique that can cope with the wide range of different grasping contexts. In other words, to mimic natural human grasping behavior as accurately as possible. 

Conventionally, there are two approaches to developing grasping strategies and algorithms. While the first one uses geometric object models, i.e. calculates a geometry-based, object-specific optimal hand posture, the second approach solely depends on tactile feedback upon contact with the object being grasped. Both approaches have the drawback that each grasp will be performed independently of the previous grasp experience. In contrast to this, humans use previous grasping information to \emph{preshape} their grasp. (The simple example of a person lying in bed at night and reaching for a glass of water as opposed to a phone or a book illustrates this.) Accordingly, more recent ideas integrate some kind of \emph{grasp experience} into the planning of the subsequent grasp \cite{Steffen}\cite{Pastor}. 

\subsection*{Grasp Preshaping}
The main idea of grasp adaptation is to use previously acquired grasping knowledge to improve future grasping strategies. One possible method we proposed was to try to equalize time-to-contact across all fingers, based on previous grasp shapes. This idea was inspired by the adaptive grasping algorithms of Humberston and Pai \cite{Ben}. However, in the course of the project we found many limitations to adapting this specific algorithm to the Barrett robotic arm and hand.

As the Barrett Hand is equipped with 1-DOF finger joints, preliminary preshaped grasps were very similar. Most of the grasping action is governed by the automatic TorqueSwitch\texttrademark\; mechanism in each finger \cite{manual}. Also, since the objects being grasped were not fixed to the pedestal, the hand had the tendency to push them around until all three fingers were making contact simultaneously.  

In addition to the problems inherent in the hardware, we faced temporary technical difficulties with respect to the torque sensor data collection. As the torque sensor readouts would have been crucial in identifying the different times of contact for each finger, we finally discarded the idea of preshaping the Barrett Hand.

\subsection*{Object Recognition}
Our other main interest was in how the properties of the object being grasped would influence the sensor output. The Barrett Hand is equipped with a rich set of sensors which cover three different modalities. This is analogous to the mechanoreceptors of the human fingertips, which themselves cover at least three different modalities: strain, vibration, and rate of change \cite{SkinTouch}. In our case, the modalities are mass (given by the force torque sensor), geometric shape (inferred from finger joint positions), and pliancy (given by tactile pressure sensors). We expect that the combination of three such orthogonal modalities will constitute a fairly unique description of an object.

Given such compelling sensor feedback, would the system be able to recognize an object from a predefined trained set? We ultimately opted to bring statistical classification to bear on this question. This approach is motivated by the idea that once the system has more high-level information about the type of object it is sensing, it can employ grasps/strategies suited to that particular type of object. A key part of using previous experiences is being able to sort and categorize those experiences.

